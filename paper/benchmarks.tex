\documentclass[journal]{vgtc}                % final (journal style)
%\documentclass[review,journal]{vgtc}         % review (journal style)
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint,journal]{vgtc}       % preprint (journal style)
%\documentclass[electronic,journal]{vgtc}     % electronic version, journal

%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier. Further, ``electronic'' includes
%% hyperreferences for more convenient online viewing.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Please note that the use of figures other than the optional teaser is not permitted on the first page
%% of the journal version.  Figures should begin on the second page and be
%% in CMYK or Grey scale format, otherwise, colour shifting may occur
%% during the printing process.  Papers submitted with figures other than the optional teaser on the
%% first page will be refused.

%% These three lines bring in essential packages: ``mathptmx'' for Type 1
%% typefaces, ``graphicx'' for inclusion of EPS figures. and ``times''
%% for proper handling of the times font family.

\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{times}

%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.

%% This turns references into clickable hyperlinks.
\usepackage[bookmarks,backref=true,linkcolor=black]{hyperref} %,colorlinks
\hypersetup{
  pdfauthor = {},
  pdftitle = {},
  pdfsubject = {},
  pdfkeywords = {},
  colorlinks=true,
  linkcolor= black,
  citecolor= black,
  pageanchor=true,
  urlcolor = black,
  plainpages = false,
  linktocpage
}

%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{0}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Research}

%% allow for this line if you want the electronic option to work properly
\vgtcinsertpkg

%% In preprint mode you may define your own headline.
%\preprinttext{To appear in an IEEE VGTC sponsored conference.}

\include{defs}

%% Paper title.

\title{A Benchmark for Interactive Data Cubes}

%% This is how authors are specified in the journal style

%% indicate IEEE Member or Student Member in form indicated below
\author{Sean Stephens, Carlos Scheidegger}
\authorfooter{
%% insert punctuation at end of each item
\item
 Sean Stephens is at the University of Arizona. 
 E-mail: seanastephens@email.arizona.edu
\item
 Carlos Scheidegger is at the University of Arizona. 
 E-mail: cscheid@email.arizona.edu \FIXME{Check this}
}

%other entries to be set up for journal
\shortauthortitle{Biv \MakeLowercase{\textit{et al.}}: Global Illumination for Fun and Profit}
%\shortauthortitle{Firstauthor \MakeLowercase{\textit{et al.}}: Paper Title}

\abstract{
	The data cube is a convenient abstraction for exploring aggregations of
	multi-dimensional datasets.  However, representations of large datasets as
	data cubes tend to require prohibitive amounts of memory or time,
	especially as the dimensionality of the data increases.  Recently, data
	cube systems have been developed specifically for interactive
	visualization.  They allow exploration of large datasets through the data
	cube abstraction at interactive speeds by maintaining an efficient 
	representation in main memory, or by using pre-aggregation and the
	GPU. We argue that existing performance measurements for these systems are
	at times incomparable, and that the performance characteristics of each are
	poorly understood.  In light of this, we present a benchmark that uses
	real-world spatiotemporal datasets as well as synthetic data to explore the
	performance of these systems on real-world and synthetic query sets.  We
	compare several existing systems using an implementation of these
	benchmarks and demonstrate performance differences between them, measured
	as startup latency, query latency, memory use, and bandwidth use.  Finally,
	we give a set of guidelines for choosing between these systems based on
	application requirements.
}

%% Keywords that describe your work. Will show as 'Index Terms' in journal
%% please capitalize first letter and insert punctuation after last keyword
\keywords{Benchmark, Data Cube, Multidimensional Data.}

%% ACM Computing Classification System (CCS). 
%% See <http://www.acm.org/class/1998/> for details.
%% The ``\CCScat'' command takes four arguments.

\CCScatlist{ % not used in journal version
 \CCScat{K.6.1}{Management of Computing and Information Systems}%
{Project and People Management}{Life Cycle};
 \CCScat{K.7.m}{The Computing Profession}{Miscellaneous}{Ethics}
}

%% Uncomment below to include a teaser figure.
%%  \teaser{
%% \centering
%% \includegraphics[width=16cm]{CypressView}
%%  \caption{In the Clouds: Vancouver from Cypress Mountain.}
%%  }

%% Uncomment below to disable the manuscript note
%\renewcommand{\manuscriptnotetxt}{}

%% Copyright space is enabled by default as required by guidelines.
%% It is disabled by the 'review' option or via the following command:
% \nocopyrightspace

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% START OF THE PAPER %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.

%% the only exception to this rule is the \firstsection command
\firstsection{Introduction}

\maketitle

%% \section{Introduction} %for journal use above \firstsection{..} instead
Data cubes are a useful tool for exploring aggregates of high dimensional data.
In particular, they allow the common \FIXME{should link to Roll up? Drill down? Sub-total?
Group-by? Cross-Tab?}\cite{2007-data-cube}.  Recent \textit{visual} data cube
systems allow exploration of multidimensional data using a visualized data
cube, allowing selections over different dimensions and showing aggregates of
the data with respect to those selections. However, implementing these visual
data cube systems efficiently enough to support large, high-dimensional
datasets with low update latency is technically difficult, and often requires large amounts of memory, bandwidth, or CPU time \cite{???}. Despite these difficulties, 
these systems is required for interactive visualizations of these datasets.

To address these problems, recent systems such as \textit{Nanocubes} and
\textit{Hashedcubes} allow fast queries in several dimensions by storing the
dataset in a server-side structure on dedicated hardware
\cite{2013-nanocubes,hashedcube}. This approach is more complex and
resource intensive than simpler approaches such as linear scans on a client,
as in \textit{datavore} and \textit{crossfilter} \cite{datavore,crossfilter}.
On the other hand, it allows interactive exploration of data sets of tens of
millions of points. Alternatively, limits on the resolution or dimensionality of the dataset can allow pre-aggregation techniques that reduce query latency to interactive rates, as in \textit{imMens} \cite{2013-immens}. 

Each of these systems were published with performance data exercising
the system on real-world data. However, the published performance measures
are not, in general, comparable. Different datasets were used, and critical
parameters such as field resolution differ. As such, there is no complete
picture of which data and schemas each of these systems is best suited for,
nor how each scales in different configurations.

This paper contributes a benchmark that allows direct comparisons of the query
performance of these systems on realistic data. We describe an implementation
for nanocubes hashedcubes, imMens, crossfilter/datavore \FIXME{We really only need one of these -- assuming that they utilize similar techniques, showing both really won't add much information}, and report
performance in terms of startup and query latency, memory consumption, and
bandwidth requirements. Finally, we give guidance for choosing between and
configuring these systems based on dataset and application.


\section{Related work}

Large data visualization frequently requires computing aggregations over the
entirety of a dataset to produce counts, histograms, and density plots
\cite{?}. For an interactive visualization, supporting arbitrary aggregations
smoothly on large datasets presents a problem due to performance. To maintain
an interactive user experience, a plot of aggregates should refresh with low
latency (This is not merely an aesthetic concern; high latency induces significant
changes in a user's exploration to the detriment of analysis, as explored in
\cite{2014-latency}). However, naive techniques for computing aggregations, namely linear scans over the dataset, do not scale
to datasets with tens of millions of entries.

In this paper we are most interested in systems that allow fast queries on data cubes built from large spatiotemporal datasets. Specifically, we mean systems that efficiently support queries with latency ideally less than 100 ms but up to 1 second (using guidance from \cite{2014-latency} on data with the following types of attributes:

\begin{itemize}
	\item $1$ or more \textit{spatial} attributes. This type is inspired by geographical location, usually latitude and longitude, but more abstractly is any pair of related coordinates from an ordered domain. Indeed, while some systems provide explicit support for latitude and longitude, others allow arbitrary domains for spatial attributes and leave the choice of encoding to the user of the system.
	\item $0$ or more \textit{categorical} attributes. Abstractly, this means a value from a discrete domain, but as a concrete example might be device type, language, or day of week for a telecommunication dataset.
	\item $1$ \textit{time} attribute. This is any attribute over an ordered domain, but is frequently time.
\end{itemize}

\subsection{Existing Systems}

Several solutions exist for this problem. Nanocubes builds a
hierarchical tree of agregations that takes advantage of redundancy in the dataset
to fit in main memory, in contrast to
traditional relational database data cube aggregations which must be stored on
disk \cite{nanocubes}. This system provides query performance that is fast
enough for interactive applications and suports queries at different
resolutions (up to the limits of the underlying data), but is memory intensive.

An alternative to nanocubes is hashedcubes. Hashedcubes 
achieves similar performance for many common queries but consumes less memory
by using a single sorted array representation of the data, at the cost of poor 
performance in some cases.
Additionally, it does not allow fine queries in sparsely populated areas, as a
tradeoff to guard against poor performance pathological queries.

\textit{ImMens} pre-computes compact aggregates in so-called
``data-tiles'' \cite{2013-immens}. These tiles can be quickly queried using the
GPU on the client.  ImMens achieves very fast response time (limited by the GPU
refresh rate) and does not require an active server \FIXME{It has a dedicated server--need to verify that it is basically just file serving}, but requires
both pre-computing data tiles and transmitting them to a client, as well as imposes limits on the resolution and dimensionality of the client's view into the dataset.

For moderate datasets with $1-5$ million elements, it is possible to do
queries at interactive latencies in a web browser. Systems such as
\textit{datavore} and \textit{crossfilter} take this approach
\cite{crossfilter,datavore}. While this approach does not scale to tens and
hundreds of millions of data points due to latency and bandwidth consumption,
it is a simple solution for small datasets, and has scales well as dimensionality increases for fixed dataset size.

As an alternative to exact representations, some systems increase performance on large datasets by sampling.
\textit{BlinkDB} uses parallelism and good choice of sampling strategy to
achieve approximate query results with $2-10\%$ error and response times on the
order of $2$ seconds \cite{blinkdb}. This system is designed for datasets on
the order of terabytes in size, but does not provide response times fast enough
for an interactive application by our definition here.
In a related technique, systems like \textit{\FIXME{SAMPLING SYSTEM}} stream data to a client interface with low initial latency, but potentially several seconds until sufficient accuracy for common decision problems \cite{trust me} 

\TODO{"Implementing Data Cubes Efficiently" might be cited here...}

For datasets with tens to hundreds of millions of
points, nanocubes, hashedcubes, imMens, and datavore can produce exact \FIXME{is exact an overstatement here?} results
using less time then these streaming/sampling systems. Furthermore, there are fundamental differences in the visualizations that sampling/streaming systems and exact systems support \cite{nivan}. For this reason, we leave a benchmark comparing sampling/streaming systems to future work, and make no claim about their suitability in application compared to exact systems. 

In this paper we will describe a benchmark aimed at nanocubes, hashedcubes,
imMens, and \FIXME{crossfilter/datavore}. Some of these systems have published performance measurements. Despite this, it is not possible to definitively state how
each of these systems perform relative to each other. It is similarly not
well-understood how each of these systems scale with dataset size
and dimensionality. Our contribution is to rectify this and provide guidance 
for choosing between these systems for applications.

\TODO{Why is TPC not applicable? Built for on-disk systems, not specifically spatiotemporal}

\section{Benchmark}

We have the following goals for a benchmark:

\begin{itemize}
	\item \textbf{Use realistic data and queries.} The systems under test
		should operate on datasets representative of real-world data. This
		means large datasets with varied schema that frequently occur in
		real-world application. The benchmark should test datasets
		that range from $1-100$ million data points, with $3$ or more dimensions
		\FIXME{Why is this the right range? What evidence do we have for
		this?}\cite{...}.  Additionally, the queries run against the systems
		under test should be realistic, and ideally be derived from queries that
		result from actual user behavior.

	\item \textbf{Exercise entire data-query space.} The set of configurations
		and query sets used by the benchmark should span the full range of data
		sets for which these systems may be reasonably applied, and completely exercise the system on each such dataset.
		
\end{itemize}

\subsection{Realistic data and queries}

In the interest of supporting further research and consistent performance measurements across systems, part of the benchmark will be based on publicly-available data sets. However, we will supplement this with synthetic data when there is no suitable public dataset for testing a particular aspect of performance.

Publicly available datasets fall into several categories in terms of schema and size:

\begin{figure*}
%% Table captions on top in journal version
 \scriptsize
 \label{benchmark-figure}
 \caption{Benchmark}
 \begin{tabular}{ p{3cm}p{2cm}p{4cm}p{4cm} }
   Data class & Size & Number of Dimensions & Candidates \\
   \hline
   Moderate & 1-10M & 1 spatial, 1 time, 0-7 categorical & Brightkite, Gowalla, Synthetic \\
   Large & 10-150M+ & 1 spatial, 1 time, 0-4 categorical & Airline, Synthetic \\ 
   Multi-spatial & 10-100M & 2 spatial, 1 time, 0-2 categorical & Airline, taxicab, synthetic\\
 \end{tabular}
 \begin{tabular}{p{3cm}p{5cm}}
   Query class & Description \\
   \hline
   User-generated & Session data from linked spatial, temporal, and categorical views \\
   Synthetic & Product of samples from possible spatial queries, categorical queries, and temporal queries\\
 \end{tabular}
\end{figure*}

\TODO{Explain why these are representative, good, etc.}

Our query sets are a combination of query sequences collected anonymously from user interactions with applications supported by nanocubes, as well synthetic query sequey.

\TODO{Explain why user traces are representative of query patterns, with a massive caveat because they were collected through a small variety of interfaces}

We use query sequences collected from users because they are, at the very least, an approximation of possible user behavior. \FIXME{Make this caveat stand out more:} A major weakness of this source of data is that the user interactions occurred in the context of a specific user interface. It is well-established that the design of a user interface greatly impacts the patterns of exploration of users \FIXME{Why} \cite{?}. At the very least, an interface without a mechanism to express a particular query will never allow that query in a user session. On the other hand, using actual user data provides important advantages over purely synthetic query sets. Indeed, actual user data may have different query types in realistic proportions. Those proportions would be difficult to ascertain without examining user data. Additionally, the order of the queries is potentially important. Ordering may have complex interactions with caching at hardware and software levels as well as software optimizations in servers or web browsers. In the absence of a better model in which to properly understand the interactions of these factors, using realistic query sequences gives us a reasonable starting place to understand the performance of these systems.

\TODO{Include screenshots of nanocube interfaces}

To augment collected query sequences, we will generate an additional list of queries. Intuitively, this additional query set will be the product of 
\begin{enumerate}
	\item A sample of square spatial queries at minimal resolution, a sample of thin rectangular spatial queries at minimal resolution, a sample of square spatial queries at high resolution, and a sample of thin rectangular spatial queries at high resolution
	\item A sample of combinations of categorical dimensions
	\item A sample of large time ranges, a sample of short time ranges.
\end{enumerate}

For the multi-spatial dataset, (1) will be applied twice. 

In contrast to user-generated queries, the synthetic queries are less realistic but potentially cover a broader range of possible queries. Thus, we use the user queries to measure performance under realistic load, and the synthetic queries to explore the rest of the query space for each system.

\TODO{Describe what we want to measure; memory, query time, build time}

The first metrics of interest during queries will be query latency and memory consumption by the server. For nanocubes and hashedcubes this means memory consumed on the server-side, while for \FIXME{crossfilter/datavore} and imMens it means browser/GPU memory consumption. Of secondary interest is startup time. For nanocubes and hashedcubes this means the time that the server takes to read in data and be ready to accept queries. For imMens and \FIXME{crossfilter/datavore} this means the bandwidth/time required for the initial data transfer to the client. We also report tile construction time for imMens. \FIXME{This all needs to be torn out and re-written}

\section{Experiments}

\begin{figure*}
%% Table captions on top in journal version
 \scriptsize
 \label{results-figure}
 \caption{Results - Either time/memory or two separate plots}
 \begin{tabular}{ p{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm} }
	 System & 
	 Moderate/User-queries & 
	 Moderate/Synthetic-queries & 
	 Large/User-queries & 
	 Large/Synthetic-queries & 
	 Multi-Spatial/Synthetic-queries \\
   \hline
   Hashedcubes &&&&& \\
   Nanocubes &&&&& \\
   ImMens &&&&& \\
   Crossfilter/Datavore &&&&& \\
 \end{tabular}
\end{figure*}

\TODO{details of experiments (compiling, machine, how many times, more detail)}

\TODO{Interpretation of results}

\section{Discussion}

\begin{figure*}
%% Table captions on top in journal version
 \scriptsize
 \label{recommendations-figure}
 \caption{Recommendations - How do we want this?}
 \begin{tabular}{ p{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm} }
 \end{tabular}
\end{figure*}
\TODO{How to improve the benchmark itself}

\TODO{Different interface/system induces different user behavior (cite Jeff Heer, The effects of interactive latency on explorative visual analysis)}

\TODO{Can be extended to new data, hopefully new queries}

\section{Conclusion and Future Work}

\TODO{What this means for making new systems (easier, negative space)}

\TODO{What next (making things parameterizable, what is the next research question...)}

% \begin{figure}[htb]
%  \centering
%  \includegraphics[width=1.5in]{sample}
%  \caption{Sample illustration.}
% \end{figure}


%% if specified like this the section will be committed in review mode
\acknowledgments{
	\TODO{Carlos}
}

\bibliographystyle{abbrv}
%%use following if all content of bibtex file should be shown
%\nocite{*}
\bibliography{benchmarks}
\end{document}
